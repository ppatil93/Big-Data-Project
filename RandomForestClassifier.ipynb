{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Priyanka\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=SparkSQL>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "os.getcwd()\n",
    "import findspark\n",
    "findspark.init('/Users/priyankapatil/anaconda3/lib/python3.7/site-packages/pyspark')\n",
    "print (\"Hey Priyanka\")\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_cancelled', 'pandas_datacancelled']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "del data\n",
    "alldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\n",
    "print(alldfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankapatil/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (0,1,2,3,4,7,10,11,12,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r'/Users/priyankapatil/Desktop/Data/dataset_flight.csv',sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"DEP_TIME\",\"ARR_TIME\",\"TAIL_NUM\",\"TAXI_OUT\",\"TAXI_IN\",\"YEAR\", \"QUARTER\",\"DAY_OF_MONTH\",\"AIR_TIME\",\"OP_UNIQUE_CARRIER\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering only flights that were cancelled\n",
    "data_cancelled=data.query(\"CANCELLED == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236794, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cancelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting columns to float\n",
    "int_cols = ['MONTH','DAY_OF_WEEK','OP_CARRIER_FL_NUM','CRS_DEP_TIME','DEP_DELAY','CRS_ARR_TIME']\n",
    "data_cancelled[int_cols] = data_cancelled[int_cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "int_cols = ['CANCELLED','DIVERTED','CRS_ELAPSED_TIME','ARR_DELAY']\n",
    "data_cancelled[int_cols] = data_cancelled[int_cols].apply(pd.to_numeric, downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['ACTUAL_ELAPSED_TIME','DISTANCE','CARRIER_DELAY','WEATHER_DELAY','NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY']\n",
    "data_cancelled[int_cols] = data_cancelled[int_cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MONTH                  float32\n",
       "DAY_OF_WEEK            float32\n",
       "OP_CARRIER_FL_NUM      float32\n",
       "ORIGIN                  object\n",
       "DEST                    object\n",
       "CRS_DEP_TIME           float32\n",
       "DEP_DELAY              float32\n",
       "CRS_ARR_TIME           float32\n",
       "ARR_DELAY              float32\n",
       "CANCELLED              float32\n",
       "CANCELLATION_CODE       object\n",
       "DIVERTED               float32\n",
       "CRS_ELAPSED_TIME       float32\n",
       "ACTUAL_ELAPSED_TIME    float32\n",
       "DISTANCE               float32\n",
       "CARRIER_DELAY          float32\n",
       "WEATHER_DELAY          float32\n",
       "NAS_DELAY              float32\n",
       "SECURITY_DELAY         float32\n",
       "LATE_AIRCRAFT_DELAY    float32\n",
       "Unnamed: 30            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cancelled.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankapatil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_cancelled['DEP_DEL15'] = np.where((data_cancelled['DEP_DELAY'] > 15),1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['DEP_DEL15']\n",
    "data_cancelled[int_cols] = data_cancelled[int_cols].apply(pd.to_numeric, downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cancelled=data_cancelled.loc[:, ~data_cancelled.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_datacancelled=data_cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Schema to convert to Spark dataframe\n",
    "schema = StructType([StructField(\"Month\", StringType(), True)\\\n",
    "                       ,StructField(\"DAY_OF_WEEK\", FloatType(), True)\\\n",
    "                       ,StructField(\"FlightNo\", FloatType(), True)\\\n",
    "                       ,StructField(\"Origin\", StringType(), True)\\\n",
    "                       ,StructField(\"Dest\", StringType(), True)\\\n",
    "                       ,StructField(\"SchDep\", FloatType(), True)\\\n",
    "                       ,StructField(\"DepDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"SchArr\", FloatType(), True)\\\n",
    "                       ,StructField(\"ArrDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"Cancelled\", FloatType(), True)\\\n",
    "                       ,StructField(\"Cancelcode\", StringType(), True)\\\n",
    "                       ,StructField(\"Diverted\", FloatType(), True)\\\n",
    "                       ,StructField(\"SchElapsedTime\", FloatType(), True)\\\n",
    "                       ,StructField(\"ElapsedTime\", FloatType(), True)\\\n",
    "                       ,StructField(\"Distance\", FloatType(), True)\\\n",
    "                       ,StructField(\"CarrierDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"WeatherDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"NASDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"SecurityDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"LateAircraftDelay\", FloatType(), True)\\\n",
    "                       ,StructField(\"DepDelay15\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+------+----+------+--------+------+--------+---------+----------+--------+--------------+-----------+--------+------------+------------+--------+-------------+-----------------+----------+\n",
      "|Month|DAY_OF_WEEK|FlightNo|Origin|Dest|SchDep|DepDelay|SchArr|ArrDelay|Cancelled|Cancelcode|Diverted|SchElapsedTime|ElapsedTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|DepDelay15|\n",
      "+-----+-----------+--------+------+----+------+--------+------+--------+---------+----------+--------+--------------+-----------+--------+------------+------------+--------+-------------+-----------------+----------+\n",
      "|  4.0|        2.0|   856.0|   LAX| IAD| 947.0|     NaN|1748.0|     NaN|      1.0|         A|     0.0|         301.0|        NaN|  2288.0|         NaN|         NaN|     NaN|          NaN|              NaN|       0.0|\n",
      "|  4.0|        2.0|   421.0|   ATL| EWR|1036.0|     NaN|1255.0|     NaN|      1.0|         A|     0.0|         139.0|        NaN|   746.0|         NaN|         NaN|     NaN|          NaN|              NaN|       0.0|\n",
      "|  4.0|        2.0|   208.0|   IAD| LAX|1905.0|     NaN|2144.0|     NaN|      1.0|         A|     0.0|         339.0|        NaN|  2288.0|         NaN|         NaN|     NaN|          NaN|              NaN|       0.0|\n",
      "|  4.0|        2.0|  1735.0|   CVG| ORD| 635.0|     NaN| 652.0|     NaN|      1.0|         A|     0.0|          77.0|        NaN|   264.0|         NaN|         NaN|     NaN|          NaN|              NaN|       0.0|\n",
      "|  4.0|        1.0|  3293.0|   SAV| JFK|1740.0|     NaN|2002.0|     NaN|      1.0|         B|     0.0|         142.0|        NaN|   718.0|         NaN|         NaN|     NaN|          NaN|              NaN|       0.0|\n",
      "+-----+-----------+--------+------+----+------+--------+------+--------+---------+----------+--------+--------------+-----------+--------+------------+------------+--------+-------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataspark = spark.createDataFrame(pandas_datacancelled, schema=schema)\n",
    "dataspark.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"SchDep\", outputCol=\"index\")\n",
    "model = stringIndexer.fit(dataspark)\n",
    "df_data = model.transform(dataspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[22] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse columns for classification(data) & Regression(datar)\n",
    "\n",
    "data = df_data.rdd.map(lambda x: ((x[10]), Vectors.dense(float(x[2]),float(x[1]),float(x[5]),float(x[7]),float(x[14]))))\n",
    "datar = df_data.rdd.map(lambda x: ((x[9]), Vectors.dense(float(x[2]),float(x[1]),float(x[5]),float(x[7]),float(x[14]))))\n",
    "datalr = df_data.rdd.map(lambda x: ((x[9]), Vectors.dense(float(x[5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "datap = sqlContext.createDataFrame(data, ['label','features'],samplingRatio=0.2)\n",
    "datapr = sqlContext.createDataFrame(datar, ['label','features'],samplingRatio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datlr = sqlContext.createDataFrame(datalr, ['label','features'],samplingRatio=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out null values\n",
    "\n",
    "dfapp1=datap.filter(\"label is not null\")\n",
    "dfapp2=datapr.filter(\"label is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfapp3=datlr.filter(\"label is not null\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\", handleInvalid=\"skip\").fit(dfapp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dfapp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets 70 and 30\n",
    "(trainingData, testData) = dfapp1.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             B|    A|[1.0,1.0,800.0,11...|\n",
      "|             B|    A|[1.0,3.0,700.0,80...|\n",
      "|             B|    A|[2.0,4.0,630.0,73...|\n",
      "|             B|    A|[2.0,4.0,900.0,17...|\n",
      "|             B|    A|[3.0,3.0,730.0,83...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.429697\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=RandomForestClassifier_8177945ba633) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "rfModel = model.stages[2]\n",
    "print(rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
